# -*- coding: utf-8 -*-

"""

@author: alexyang

@contact: alex.yang0326@gmail.com

@file: preprocess.py

@time: 2019/5/8 20:37

@desc:

"""

import os
import json
from collections import defaultdict
from tqdm import tqdm
import jieba
import numpy as np
from config import PROCESSED_DATA_DIR, LOG_DIR, SUBMIT_DIR, MODEL_SAVED_DIR, KB_FILENAME, MENTION_TO_ENTITY_FILENAME, \
    ENTITY_TO_MENTION_FILENAME, ENTITY_DESC_FILENAME, ENTITY_TYPE_FILENAME, CCKS_TRAIN_FILENAME, VOCABULARY_TEMPLATE, \
    IDX2TOKEN_TEMPLATE, TRAIN_DATA_FILENAME, DEV_DATA_FILENAME, TEST_DATA_FILENAME, CCKS_TEST_FILENAME, \
    EMBEDDING_MATRIX_TEMPLATE, CCKS_TEST_FINAL_FILENAME, TEST_FINAL_DATA_FILENAME, IMG_DIR
from utils.io import pickle_dump, format_filename
from utils.embedding import train_w2v, train_glove, train_fasttext


def load_kb_data(kb_file):
    """process knowledge base file"""
    mention_to_entity = defaultdict(list)
    entity_to_mention = defaultdict(list)
    entity_desc = defaultdict()
    entity_type = defaultdict(list)
    with open(kb_file) as reader:
        for line in tqdm(reader):
            kb_data = json.loads(line)
            entity_id = kb_data['subject_id']

            desc = '\n'.join('%s：%s' % (data['predicate'], data['object']) for data in kb_data['data'])
            desc.lower() # 实体描述信息，每行一个data里面的【属性 属性值】，换行符分隔
            if not desc:
                continue
            entity_desc[entity_id] = desc

            mentions = list(set(kb_data.get('alias', []) + [kb_data['subject']])) # alias和subject都作为mention
            mentions = [mention.lower() for mention in mentions]
            entity_to_mention[entity_id] = mentions

            for mention in mentions:
                if entity_id not in mention_to_entity[mention]:
                    mention_to_entity[mention].append(entity_id) # 每一个mention对应若干entity_id

            for _type in kb_data['type']:
                entity_type[entity_id].append(_type)

    return mention_to_entity, entity_to_mention, entity_desc, entity_type

# 简单处理：转小写，去除NIL，改正offset错误标注，去除offset为-1
# 最后train_data里面只有text和mention
def load_train_data(erl_file):
    train_data = []
    with open(erl_file) as reader:
        for line in tqdm(reader):
            data = json.loads(line)
            erl_text = data['text'].lower()

            mention_data = []
            for x in data['mention_data']:
                mention_text, offset, entity = x['mention'].lower(), int(x['offset']), x['kb_id']
                if entity == 'NIL':
                    continue
                if erl_text[offset: offset + len(mention_text)] != mention_text:
                    offset = erl_text.find(mention_text)
                if offset == -1:
                    continue
                mention_data.append((mention_text, offset, entity))
            train_data.append({'text': erl_text, 'mention_data': mention_data})
    return train_data

# test_data存储了：text_id，原始文本，转小写后的文本
def load_test_data(erl_file):
    test_data = []
    with open(erl_file) as reader:
        for line in tqdm(reader):
            data = json.loads(line)
            test_data.append({'text_id': data['text_id'], 'text': data['text'].lower(), 'raw_text': data['text']})
    return test_data

# 根据实体描述信息和训练数据构建词典和语料
# char2idx(我：3), idx2char（3：我）, corpus（每一条desc/训练文本都作为语料的一句，每一条由单字列表组成）
# 注意：char2idx和idx2char都做了最低频次过滤，但corpus内部保留了所有原始信息
def load_char_vocab_and_corpus(entity_desc, train_data, min_count=2):
    chars = dict()
    corpus = []
    for desc in tqdm(iter(entity_desc.values())):
        for c in desc:
            chars[c] = chars.get(c, 0) + 1# 统计每个字符出现的次数，如"我：33"
        corpus.append(list(desc))#每一个句话都作为语料的一个元素，每句话由单字列表组成
    for data in tqdm(iter(train_data)):
        for c in data['text']:
            chars[c] = chars.get(c, 0) + 1#训练数据中的字也计入字出现频次
        corpus.append(list(data['text']))
    chars = {i: j for i, j in chars.items() if j >= min_count}#至少出现2次才保留
    idx2char = {i + 2: j for i, j in enumerate(chars)}  # 0: mask, 1: padding # 枚举的j为字典char的key，即字符
    char2idx = {j: i for i, j in idx2char.items()}
    return char2idx, idx2char, corpus


def load_bichar_vocab_and_corpus(entity_desc, train_data, min_count=2):
    bichars = dict()
    corpus = []
    for desc in tqdm(iter(entity_desc.values())):
        bigrams = []
        for i in range(len(desc)):
            c = desc[i] + '</end>' if i == len(desc) - 1 else desc[i:i+2]
            bigrams.append(c) # 一句话中的所有二元组合
            bichars[c] = bichars.get(c, 0) + 1 #每一个二元组合的频次统计
        corpus.append(bigrams) #每一个句话都作为语料的一个元素，每句话由二元组合的列表组成
    for data in tqdm(iter(train_data)):
        bigrams = []
        for i in range(len(data['text'])):
            c = data['text'][i] + '</end>' if i == len(data['text']) - 1 else data['text'][i:i+2]
            bigrams.append(c)
            bichars[c] = bichars.get(c, 0) + 1
        corpus.append(bigrams)
    bichars = {i: j for i, j in bichars.items() if j >= min_count}
    idx2bichar = {i + 2: j for i, j in enumerate(bichars)}  # 0: mask, 1: padding
    bichar2idx = {j: i for i, j in idx2bichar.items()}
    return bichar2idx, idx2bichar, corpus


def load_word_vocab_and_corpus(entity_desc, train_data, min_count=2):
    words = dict()
    corpus = []
    for desc in tqdm(iter(entity_desc.values())):
        desc_cut = jieba.lcut(desc) #jieba默认分词，返回list
        for w in desc_cut:
            words[w] = words.get(w, 0) + 1 # 统计每个词词频
        corpus.append(desc_cut)
    for data in tqdm(iter(train_data)):
        text_cut = jieba.lcut(data['text'])
        for w in text_cut:
            words[w] = words.get(w, 0) + 1
        corpus.append(text_cut)
    words = {i: j for i, j in words.items() if j >= min_count}
    idx2word = {i + 2: j for i, j in enumerate(words)}  # 0: mask, 1: padding
    word2idx = {j: i for i, j in idx2word.items()}
    return word2idx, idx2word, corpus


def load_charpos_vocab_and_corpus(char2idx, entity_desc, train_data):
    """build position aware character vocabulary by assign 4 positional tags: <B> <M> <E> <S>"""
    charpos2idx = {'<B>': 2, '<M>': 3, '<E>': 4, '<S>': 5} # 同样预留2个索引给mask,unk
    for c in char2idx.keys():
        charpos2idx[c+'<B>'] = len(charpos2idx) + 2
        charpos2idx[c+'<M>'] = len(charpos2idx) + 2
        charpos2idx[c+'<E>'] = len(charpos2idx) + 2# 带位置标签的单字，到索引的映射
        charpos2idx[c+'<S>'] = len(charpos2idx) + 2 # 扩充后变为{'<B>':2,'<M>':3,'<E>':4,'<S>':5,'我<B>':6,'我<M>':7, '我<E>':8, '我<S>': 9,……}
    idx2charpos = dict((idx, c) for c, idx in charpos2idx.items())

    corpus = []
    for desc in tqdm(iter(entity_desc.values())):# 实体描述信息，每行一个data里面的【属性 属性值】，换行符分隔
        desc_cut = jieba.lcut(desc)
        desc_pos = []
        for word in desc_cut:
            if len(word) == 1:
                desc_pos.append(word+'<S>')     # single character as one word
            else:
                for i in range(len(word)):
                    if i == 0:
                        desc_pos.append(word[i]+'<B>')  # begin
                    elif i == len(word) - 1:
                        desc_pos.append(word[i]+'<E>')  # end
                    else:
                        desc_pos.append(word[i]+'<M>')  # middle
        corpus.append(desc_pos) # 带位置标签的语料
    for data in tqdm(iter(train_data)):
        text_cut = jieba.lcut(data['text'])
        text_pos = []
        for word in text_cut:
            if len(word) == 1:
                text_pos.append(word + '<S>')  # single character as one word
            else:
                for i in range(len(word)):
                    if i == 0:
                        text_pos.append(word[i] + '<B>')  # begin
                    elif i == len(word) - 1:
                        text_pos.append(word[i] + '<E>')  # end
                    else:
                        text_pos.append(word[i] + '<M>')  # middle
        corpus.append(text_pos)
    return charpos2idx, idx2charpos, corpus


def train_valid_split(train_data):
    random_order = list(range(len(train_data)))
    np.random.shuffle(random_order)

    dev_data = [train_data[j] for i, j in enumerate(random_order) if i % 9 == 0]
    train_data = [train_data[j] for i, j in enumerate(random_order) if i % 9 != 0]

    return train_data, dev_data


if __name__ == '__main__':
    # create directory
    if not os.path.exists(PROCESSED_DATA_DIR):
        os.makedirs(PROCESSED_DATA_DIR)
    if not os.path.exists(LOG_DIR):
        os.makedirs(LOG_DIR)
    if not os.path.exists(MODEL_SAVED_DIR):
        os.makedirs(MODEL_SAVED_DIR)
    if not os.path.exists(SUBMIT_DIR):
        os.makedirs(SUBMIT_DIR)
    if not os.path.exists(IMG_DIR):
        os.makedirs(IMG_DIR)

    # load knowledge base data
    mention_to_entity, entity_to_mention, entity_desc, entity_type = load_kb_data(KB_FILENAME)
    pickle_dump(format_filename(PROCESSED_DATA_DIR, MENTION_TO_ENTITY_FILENAME), mention_to_entity)
    pickle_dump(format_filename(PROCESSED_DATA_DIR, ENTITY_DESC_FILENAME), entity_desc)
    pickle_dump(format_filename(PROCESSED_DATA_DIR, ENTITY_TYPE_FILENAME), entity_type)
    pickle_dump(format_filename(PROCESSED_DATA_DIR, ENTITY_TO_MENTION_FILENAME), entity_to_mention)

    # load training data
    train_data = load_train_data(CCKS_TRAIN_FILENAME)

    # prepare character embedding
    char_vocab, idx2char, char_corpus = load_char_vocab_and_corpus(entity_desc, train_data)
    pickle_dump(format_filename(PROCESSED_DATA_DIR, VOCABULARY_TEMPLATE, level='char'), char_vocab)
    pickle_dump(format_filename(PROCESSED_DATA_DIR, IDX2TOKEN_TEMPLATE, level='char'), idx2char)
    c2v = train_w2v(char_corpus, char_vocab)
    c_fastext = train_fasttext(char_corpus, char_vocab)
    c_glove = train_glove(char_corpus, char_vocab)
    np.save(format_filename(PROCESSED_DATA_DIR, EMBEDDING_MATRIX_TEMPLATE, type='c2v'), c2v)
    np.save(format_filename(PROCESSED_DATA_DIR, EMBEDDING_MATRIX_TEMPLATE, type='c_fasttext'), c_fastext)
    np.save(format_filename(PROCESSED_DATA_DIR, EMBEDDING_MATRIX_TEMPLATE, type='c_glove'), c_glove)

    # prepare bigram embedding
    bichar_vocab, idx2bichar, bichar_corpus = load_bichar_vocab_and_corpus(entity_desc, train_data)
    pickle_dump(format_filename(PROCESSED_DATA_DIR, VOCABULARY_TEMPLATE, level='bichar'), bichar_vocab)
    pickle_dump(format_filename(PROCESSED_DATA_DIR, IDX2TOKEN_TEMPLATE, level='bichar'), idx2bichar)
    bic2v = train_w2v(bichar_corpus, bichar_vocab, embedding_dim=50)
    bic_fastext = train_fasttext(bichar_corpus, bichar_vocab, embedding_dim=50)
    bic_glove = train_glove(bichar_corpus, bichar_vocab, embedding_dim=50)
    np.save(format_filename(PROCESSED_DATA_DIR, EMBEDDING_MATRIX_TEMPLATE, type='bic2v'), bic2v)
    np.save(format_filename(PROCESSED_DATA_DIR, EMBEDDING_MATRIX_TEMPLATE, type='bic_fasttext'), bic_fastext)
    np.save(format_filename(PROCESSED_DATA_DIR, EMBEDDING_MATRIX_TEMPLATE, type='bic_glove'), bic_glove)

    for mention in mention_to_entity.keys(): # kb_data里面的alias和subject都作为mention，加入自定义词典
        jieba.add_word(mention, freq=1000000)
    # prepare word embedding
    word_vocab, idx2word, word_corpus = load_word_vocab_and_corpus(entity_desc, train_data)
    pickle_dump(format_filename(PROCESSED_DATA_DIR, VOCABULARY_TEMPLATE, level='word'), word_vocab)
    pickle_dump(format_filename(PROCESSED_DATA_DIR, IDX2TOKEN_TEMPLATE, level='word'), idx2word)
    w2v = train_w2v(word_corpus, word_vocab)
    w_fastext = train_fasttext(word_corpus, word_vocab)
    w_glove = train_glove(word_corpus, word_vocab)
    np.save(format_filename(PROCESSED_DATA_DIR, EMBEDDING_MATRIX_TEMPLATE, type='w2v'), w2v)
    np.save(format_filename(PROCESSED_DATA_DIR, EMBEDDING_MATRIX_TEMPLATE, type='w_fasttext'), w_fastext)
    np.save(format_filename(PROCESSED_DATA_DIR, EMBEDDING_MATRIX_TEMPLATE, type='w_glove'), w_glove)

    # prepare position-based character embedding
    charpos_vocab, idx2charpos, charpos_corpus = load_charpos_vocab_and_corpus(char_vocab, entity_desc, train_data)
    pickle_dump(format_filename(PROCESSED_DATA_DIR, VOCABULARY_TEMPLATE, level='charpos'), charpos_vocab)
    pickle_dump(format_filename(PROCESSED_DATA_DIR, IDX2TOKEN_TEMPLATE, level='charpos'), idx2charpos)
    cpos2v = train_w2v(charpos_corpus, charpos_vocab)
    cpos_fastext = train_fasttext(charpos_corpus, charpos_vocab)
    cpos_glove = train_glove(charpos_corpus, charpos_vocab)
    np.save(format_filename(PROCESSED_DATA_DIR, EMBEDDING_MATRIX_TEMPLATE, type='cpos2v'), cpos2v)
    np.save(format_filename(PROCESSED_DATA_DIR, EMBEDDING_MATRIX_TEMPLATE, type='cpos_fasttext'), cpos_fastext)
    np.save(format_filename(PROCESSED_DATA_DIR, EMBEDDING_MATRIX_TEMPLATE, type='cpos_glove'), cpos_glove)

    # hold out split
    train_data, dev_data = train_valid_split(train_data) # 从train.json里面分出一部分做验证集
    test_data = load_test_data(CCKS_TEST_FILENAME) # develop.json当测试集
    pickle_dump(format_filename(PROCESSED_DATA_DIR, TRAIN_DATA_FILENAME), train_data)
    pickle_dump(format_filename(PROCESSED_DATA_DIR, DEV_DATA_FILENAME), dev_data)
    pickle_dump(format_filename(PROCESSED_DATA_DIR, TEST_DATA_FILENAME), test_data)

    # load test data
    test_final_data = load_test_data(CCKS_TEST_FINAL_FILENAME) # B榜测试集eval.json最终测试
    pickle_dump(format_filename(PROCESSED_DATA_DIR, TEST_FINAL_DATA_FILENAME), test_final_data)
